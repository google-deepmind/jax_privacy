{
  "cells": [
    {
      "metadata": {
        "id": "6y958y4LvRb7"
      },
      "cell_type": "markdown",
      "source": [
        "<div style=\"margin-bottom: 1em;\">\n",
        "  <a href=\"https://colab.research.google.com/github/google-deepmind/jax_privacy/blob/main/examples/dp_sgd_flax_linen_mnist.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" />\n",
        "  </a>\n",
        "  <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/examples/dp_sgd_flax_linen_mnist.ipynb\" target=\"_blank\" style=\"margin-left: 10px;\">\n",
        "    <img src=\"https://img.shields.io/badge/GitHub-view--source-black?logo=github\" />\n",
        "  </a>\n",
        "</div>"
      ]
    },
    {
      "metadata": {
        "id": "nKRIApHooE1i"
      },
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HnYgGmIsZ_cT"
      },
      "cell_type": "markdown",
      "source": [
        "# DP-SGD tutorial using Flax Linen on MNIST\n",
        "\n",
        "**Copyright 2025 DeepMind Technologies Limited.**\n",
        "\n",
        "Welcome to Jax Privacy for Flax Linen! In this tutorial you will learn how to train a simple CNN model in a differentially-private (DP) way using DP-SGD algorithm. We will train our model on the MNIST dataset.\n",
        "\n",
        "This turorial is based on the [official MNIST example for Flax Linen](https://github.com/google/flax/blob/main/examples/mnist/train.py). However, the code is rearranged the same way as in the [official MNIST example for Flax **NNX**](https://colab.research.google.com/github/google/flax/blob/main/docs_nnx/mnist_tutorial.ipynb).\n",
        "\n",
        "In the tutorial we highlight the changes we need to make in the official non-DP example to make the model differentially-private."
      ]
    },
    {
      "metadata": {
        "id": "G3wqHQg6f_JG"
      },
      "cell_type": "markdown",
      "source": [
        "## Install libraries\n",
        "\n",
        "If one of the following libraries are not installed in your Python environment, use pip to install the package from PyPI (below, just uncomment the code in the cell if you are working from Google Colab/Jupyter Notebook):"
      ]
    },
    {
      "metadata": {
        "id": "Ajj40XuZblzw"
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install flax\n",
        "!pip install jaxtyping\n",
        "!pip install dp_accounting\n",
        "\n",
        "!pip install jax_privacy==1.0.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "nXrfdOwff_JH"
      },
      "cell_type": "markdown",
      "source": [
        "## Define hyper-parameters\n",
        "\n",
        "First we define hyper-parameters for our training. These parameters are important for DP-SGD training. See the comments describing what each parameter means.\n",
        "\n",
        "Note, that in real applications the same hyper-parameters used for non-DP training might be not optimal for DP training. Therefore fine-tuning of these params might be necessary.\n",
        "\n",
        "With this setup, expected DP-SGD result is ~92% accuracy on the test dataset and expected non-DP result is ~99%.\n"
      ]
    },
    {
      "metadata": {
        "id": "4zMInN-sf_JH"
      },
      "cell_type": "code",
      "source": [
        "from jax_privacy.dp_sgd import grad_clipping\n",
        "\n",
        "# Whether to train model using DP-SGD or not.\n",
        "# Switch it to False to see the non-DP-SGD results for comparison.\n",
        "# Expected DP-SGD accuracy: ~92%\n",
        "# Expected non-DP accuracy: ~99%.\n",
        "use_dp = True\n",
        "\n",
        "# Training with DP-SGD might require different values for hyperparameters.\n",
        "# How many optimization steps to do.\n",
        "train_steps = 5000 if use_dp else 5000\n",
        "\n",
        "# How often (number of steps) to evaluate model performance during training.\n",
        "eval_every = 200\n",
        "\n",
        "# Batch size for training. In our example, we don't accumulate gradients, so this\n",
        "# is the batch size for each gradient update.\n",
        "# In DP-SGD the batch size matters a lot, the bigger batch size the better\n",
        "# model performance you will get spending the same amount of privacy budget.\n",
        "batch_size = 256 if use_dp else 128\n",
        "\n",
        "# Learning rate for the optimizer.\n",
        "learning_rate = 0.1 if use_dp else 0.1\n",
        "\n",
        "# Momentum for the optimizer.\n",
        "momentum = 0.9 if use_dp else 0.9\n",
        "\n",
        "# DP-SGD parameters.\n",
        "# Epsilon DP parameter.\n",
        "epsilon = 1.0\n",
        "\n",
        "# Delta DP parameter.\n",
        "delta = 1e-5\n",
        "\n",
        "# Clipping norm for the gradient vector (i.e. all gradients will have L2 norm at most equal to this value).\n",
        "# Clipping happens separately for each example in the batch (i.e. before taking sum or mean of gradients).\n",
        "clipping_norm = 0.1\n",
        "\n",
        "# Defines how to clip the gradients per each example in the batch.\n",
        "# It does not affect the results but it allows to do speed/memory trade-offs.\n",
        "# We use vectorized which requires more memory but is faster because it uses vmap and each example is clipped in parallel.\n",
        "# There is also UNROLLED which is slower but uses less memory because it uses lax.scan and each example is summed and clipped sequentially.\n",
        "per_example_grad_method = grad_clipping.VECTORIZED"
      ],
      "outputs": [],
      "execution_count": 145
    },
    {
      "metadata": {
        "id": "MPBxamhgf_JH"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the MNIST dataset\n",
        "\n",
        "First, you need to load the MNIST dataset and then prepare the training and testing sets via Tensorflow Datasets (TFDS). You normalize image values, shuffle the data and divide it into batches, and prefetch samples to enhance performance.\n",
        "\n",
        "No changes related to DP."
      ]
    },
    {
      "metadata": {
        "id": "9KiZ1oxlf_JH"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
        "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
        "\n",
        "tf.random.set_seed(0)  # Set the random seed for reproducibility.\n",
        "\n",
        "train_ds: tf.data.Dataset = tfds.load('mnist', split='train')\n",
        "test_ds: tf.data.Dataset = tfds.load('mnist', split='test')\n",
        "\n",
        "# Train size is important for DP-SGD.\n",
        "train_size = train_ds.cardinality().numpy()\n",
        "print(f'Train size: {train_size}')\n",
        "test_size = test_ds.cardinality().numpy()\n",
        "print(f'Test size: {test_size}')\n",
        "\n",
        "train_ds = train_ds.map(\n",
        "    lambda sample: {\n",
        "        'image': tf.cast(sample['image'], tf.float32) / 255,\n",
        "        'label': sample['label'],\n",
        "    }\n",
        ")  # normalize train set\n",
        "test_ds = test_ds.map(\n",
        "    lambda sample: {\n",
        "        'image': tf.cast(sample['image'], tf.float32) / 255,\n",
        "        'label': sample['label'],\n",
        "    }\n",
        ")  # Normalize the test set.\n",
        "\n",
        "# Create a shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from.\n",
        "train_ds = train_ds.repeat().shuffle(1024)\n",
        "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
        "train_ds = (\n",
        "    train_ds.batch(batch_size, drop_remainder=True)\n",
        "    .take(train_steps)\n",
        "    .prefetch(1)\n",
        ")\n",
        "# Group into batches of `batch_size` and skip incomplete batches, prefetch the next sample to improve latency.\n",
        "test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)"
      ],
      "outputs": [],
      "execution_count": 146
    },
    {
      "metadata": {
        "id": "cGZqLWiwTMxS"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the CNN model with Flax Linen\n",
        "\n",
        "No changes related to DP."
      ]
    },
    {
      "metadata": {
        "id": "75AgdOKgf_JH"
      },
      "cell_type": "code",
      "source": [
        "from flax import linen as nn\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "  \"\"\"A simple CNN model.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = x.reshape((x.shape[0], -1))  # flatten\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=10)(x)\n",
        "    return x"
      ],
      "outputs": [],
      "execution_count": 147
    },
    {
      "metadata": {
        "id": "G-CGa7N3TMxS"
      },
      "cell_type": "markdown",
      "source": [
        "## Create model and Flax Linen train state\n",
        "\n",
        "The only change is that we create two models: one we will train with DP and the other without to compare the performance."
      ]
    },
    {
      "metadata": {
        "id": "1QiCRBKdTMxS"
      },
      "cell_type": "code",
      "source": [
        "from flax.training import train_state\n",
        "from jax import random\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "\n",
        "def create_train_state(rng):\n",
        "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "  cnn = CNN()\n",
        "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "  tx = optax.sgd(learning_rate, momentum)\n",
        "  return train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)\n",
        "\n",
        "\n",
        "train_state = create_train_state(random.key(0))"
      ],
      "outputs": [],
      "execution_count": 148
    },
    {
      "metadata": {
        "id": "RxbE1O25TMxS"
      },
      "cell_type": "markdown",
      "source": [
        "## Create DP-SGD gradient computer\n",
        "\n",
        "At first we find standard deviation of Gaussian noise (`noise_multiplier`) we have to add to achieve the required privacy guarantees (defined by `eps` and `delta`).\n",
        "\n",
        "Then with `noise_multiplier` parameter we create DP-SGD `GradientComputer`. We will use this object to clip gradients and add noise."
      ]
    },
    {
      "metadata": {
        "id": "GL0Qd8UnTMxS"
      },
      "cell_type": "code",
      "source": [
        "from jax_privacy.accounting import accountants, analysis, calibrate\n",
        "from jax_privacy.dp_sgd import gradients\n",
        "\n",
        "# Calculate noise_multiplier (stddev) given the privacy budget.\n",
        "accountant = analysis.DpsgdTrainingAccountant(\n",
        "    dp_accountant_config=accountants.PldAccountantConfig()\n",
        ")\n",
        "noise_multiplier = calibrate.calibrate_noise_multiplier(\n",
        "    target_epsilon=epsilon,\n",
        "    accountant=accountant,\n",
        "    batch_sizes=batch_size,\n",
        "    num_updates=train_steps,\n",
        "    num_samples=train_size,\n",
        "    target_delta=delta,\n",
        ")\n",
        "print(f'Noise multiplier {noise_multiplier}')\n",
        "# Create gradient computer that will clip grads and add noise to them.\n",
        "gradient_computer = gradients.DpsgdGradientComputer(\n",
        "    clipping_norm=clipping_norm,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    # Simplifies learning-rate tuning, see https://arxiv.org/abs/2204.13650.\n",
        "    rescale_to_unit_norm=True,\n",
        "    per_example_grad_method=per_example_grad_method,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 149
    },
    {
      "metadata": {
        "id": "QZG5aCuKTMxS"
      },
      "cell_type": "markdown",
      "source": [
        "## Define loss function\n",
        "\n",
        "No changes related to DP."
      ]
    },
    {
      "metadata": {
        "id": "W0MtuXRCTMxS"
      },
      "cell_type": "code",
      "source": [
        "def loss_fn(params, state, batch):\n",
        "  logits = state.apply_fn({'params': params}, batch['image'])\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=batch['label']\n",
        "  ).mean()\n",
        "  return loss, logits"
      ],
      "outputs": [],
      "execution_count": 150
    },
    {
      "metadata": {
        "id": "KPggzkyQTMxS"
      },
      "cell_type": "markdown",
      "source": [
        "## Define evaluation function\n",
        "\n",
        "No changes related to DP."
      ]
    },
    {
      "metadata": {
        "id": "zdIuqNRxTMxS"
      },
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(state, batch):\n",
        "  loss, logits = loss_fn(state.params, state, batch)\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == batch['label'])\n",
        "  return loss, accuracy"
      ],
      "outputs": [],
      "execution_count": 151
    },
    {
      "metadata": {
        "id": "kSRkb_7iYiWw"
      },
      "cell_type": "markdown",
      "source": [
        "## Define non-DP train step\n",
        "\n",
        "We will use it to train non-DP model for comparison."
      ]
    },
    {
      "metadata": {
        "id": "dIjLDYy9YiWw"
      },
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def non_dp_train_step(state, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "\n",
        "  value_and_grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = value_and_grad_fn(state.params, state, batch)\n",
        "  new_state = state.apply_gradients(grads=grads)\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == batch['label'])\n",
        "  return new_state, {'loss': loss, 'accuracy': accuracy}"
      ],
      "outputs": [],
      "execution_count": 152
    },
    {
      "metadata": {
        "id": "fd4GWDVkTMxS"
      },
      "cell_type": "markdown",
      "source": [
        "## Define DP-SGD train step\n",
        "\n",
        "This is the main function where DP-SGD magic happens.\n",
        "\n",
        "First we define a loss function (`train_loss_fn`) to pass it to gradient computer. This function has to accept 4 arguments:\n",
        "* params: ArrayTree representing model weights\n",
        "* network_state: ArrayTree representing additional parameters that control the behavior of the network but are not updated via gradient, e.g. non-trainable parameters of Batch Norm (not used in our example).\n",
        "* rng_per_example: PRNGKey to generate random number per each example, e.g. to implement dropout or diffusion models (not used in our example).\n",
        "* inputs: ArrayTree repesenting the inputs (each leaf should contain an array with batch dimension)\n",
        "\n",
        "`train_loss_fn` is defined inside `train_step` to capture `state` and pass it to the standard `loss_fn` defined above. After calling `loss_fn` we get loss and logits. The loss we return as it is and logits we put into metrics per-example dictionary. `Per-example` here means that the results will be stacked over batch dimension. This is exactly what we want to make predictions and calculate the accuracy later. Other options in metrics are `scalars_avg` (average over batch dimension), `scalars_sum` (same as average but sum) and `scalars_last` (take last from per-example results).\n",
        "\n",
        "Secondly, we calculate clipped gradients by calling `gradient_computer.loss_and_clipped_gradients`. We pass our `train_loss_fn` and then arguments to be forward to `train_loss_fn`. The gradient computer will split the batch on per-example arguments (i.e. batch size will be 1) and call `train_loss_fn` for each example. The return gradients will be clipped and then averaged. `gradient_computer.loss_and_clipped_gradients` will return mean loss, network state (ignored in the example) and metrics.\n",
        "\n",
        "Now we have clipped the gradients and are ready to make an optimizer step. However, to make it differentially-private we also need to add noise to the gradients vector. We do that by calling `gradient_computer.add_noise_to_grads` where we pass gradient vector, noise PRNGKey, total batch size (i.e. total number of examples accumulated in the gradient since the last optimizer step) and the noise state. `gradient_computer.add_noise_to_grads` will return gradients vector with added noise, used standard deviation of the noise for monitoring purposes (ignored in our case) and new noise state that we have to save and pass in the next call.\n",
        "\n",
        "Then the rest of the code is the same as in the usual non-DP training: we calculate updated model weights by applying the calculating gradients with the help of optimizer and calculate the accuracy of predicions for monitoring purposes."
      ]
    },
    {
      "metadata": {
        "id": "F5FkPthzTMxS"
      },
      "cell_type": "code",
      "source": [
        "from jax_privacy.dp_sgd import typing as jax_privacy_typing\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def dp_sgd_train_step(state, batch, noise_state, noise_rng):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "\n",
        "  def train_loss_fn(params, unused_network_state, unused_rng, inputs):\n",
        "    loss, logits = loss_fn(params, state, inputs)\n",
        "    metrics = jax_privacy_typing.Metrics(per_example={'logits': logits})\n",
        "    return loss, (unused_network_state, metrics)\n",
        "\n",
        "  # ArrayTree representing additional state of the network (not used).\n",
        "  unused_network_state = {}\n",
        "  # PRNGKey to generate random number per each example (not used).\n",
        "  unused_rng = random.PRNGKey(0)\n",
        "  (loss, (_, metrics)), grads = gradient_computer.loss_and_clipped_gradients(\n",
        "      loss_fn=train_loss_fn,\n",
        "      params=state.params,\n",
        "      network_state=unused_network_state,\n",
        "      rng_per_local_microbatch=unused_rng,\n",
        "      inputs=batch,\n",
        "  )\n",
        "  noisy_grads, _, new_noise_state = gradient_computer.add_noise_to_grads(\n",
        "      grads, noise_rng, jnp.asarray(batch_size), noise_state\n",
        "  )\n",
        "  new_state = state.apply_gradients(grads=noisy_grads)\n",
        "  logits = metrics.per_example['logits']\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == batch['label'])\n",
        "  return new_state, new_noise_state, {'loss': loss, 'accuracy': accuracy}"
      ],
      "outputs": [],
      "execution_count": 153
    },
    {
      "metadata": {
        "id": "BkFJGoH1TMxS"
      },
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "The only difference from usual non-DP training is that we keep splitting the PRNGKey for the noise and keep the noise state.\n",
        "\n",
        "Worth noting that if you train with DP-SGD you can't call `train` of the same model for the second time because the `noise_multiplier` was calculated for exactly that number of train steps."
      ]
    },
    {
      "metadata": {
        "id": "Rk_7mgKYTMxS"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Train loop\n",
        "metrics_history = {\n",
        "    'train_loss': [],\n",
        "    'train_accuracy': [],\n",
        "    'test_loss': [],\n",
        "    'test_accuracy': [],\n",
        "}\n",
        "\n",
        "checkpoint_start = time.time()\n",
        "accumulated_losses = []\n",
        "accumulated_accuracies = []\n",
        "noise_state = {}\n",
        "noise_rng = random.PRNGKey(42)\n",
        "for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
        "  if use_dp:\n",
        "    rng_grads, noise_rng = random.split(noise_rng)\n",
        "    train_state, noise_state, step_metrics = dp_sgd_train_step(\n",
        "        train_state, batch, noise_state, rng_grads\n",
        "    )\n",
        "  else:\n",
        "    train_state, step_metrics = non_dp_train_step(train_state, batch)\n",
        "  accumulated_losses.append(step_metrics['loss'])\n",
        "  accumulated_accuracies.append(step_metrics['accuracy'])\n",
        "\n",
        "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):\n",
        "    checkpoint_time = time.time() - checkpoint_start\n",
        "    # Log the training metrics.\n",
        "    metrics_history[f'train_loss'].append(np.mean(accumulated_losses))\n",
        "    metrics_history[f'train_accuracy'].append(np.mean(accumulated_accuracies))\n",
        "    accumulated_losses = []\n",
        "    accumulated_accuracies = []\n",
        "\n",
        "    # Compute the metrics on the test set.\n",
        "    for test_batch in test_ds.as_numpy_iterator():\n",
        "      loss, accuracy = eval_step(train_state, test_batch)\n",
        "      accumulated_losses.append(loss)\n",
        "      accumulated_accuracies.append(accuracy)\n",
        "\n",
        "    # Log the test metrics.\n",
        "    metrics_history[f'test_loss'].append(np.mean(accumulated_losses))\n",
        "    metrics_history[f'test_accuracy'].append(np.mean(accumulated_accuracies))\n",
        "    accumulated_losses = []\n",
        "    accumulated_accuracies = []\n",
        "\n",
        "    print(\n",
        "        f' [elapsed time]: {checkpoint_time:.2f}\\n',\n",
        "        f'[train] step: {step}, '\n",
        "        f'loss: {metrics_history[\"train_loss\"][-1]}, '\n",
        "        f'accuracy: {metrics_history[\"train_accuracy\"][-1] * 100}',\n",
        "    )\n",
        "    print(\n",
        "        f' [test] step: {step}, '\n",
        "        f'loss: {metrics_history[\"test_loss\"][-1]}, '\n",
        "        f'accuracy: {metrics_history[\"test_accuracy\"][-1] * 100}'\n",
        "    )\n",
        "    checkpoint_start = time.time()"
      ],
      "outputs": [],
      "execution_count": 154
    },
    {
      "metadata": {
        "id": "FEPvh0YWZ_cT"
      },
      "cell_type": "markdown",
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
