{
  "cells": [
    {
      "metadata": {
        "id": "MtgkME1kEPQv"
      },
      "cell_type": "markdown",
      "source": [
        "<div style=\"margin-bottom: 1em;\">\n",
        "  <a href=\"https://colab.research.google.com/github/google-deepmind/jax_privacy/blob/main/examples/dp_sgd_keras_gemma3_synthetic_data.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" />\n",
        "  </a>\n",
        "  <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/examples/dp_sgd_keras_gemma3_synthetic_data.ipynb\" target=\"_blank\" style=\"margin-left: 10px;\">\n",
        "    <img src=\"https://img.shields.io/badge/GitHub-view--source-black?logo=github\" />\n",
        "  </a>\n",
        "</div>"
      ]
    },
    {
      "metadata": {
        "id": "0FD2FkTfN7-o"
      },
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "fe06Ss0p9UVA"
      },
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Generating Differentially Private Synthetic Data\n",
        "\n",
        "**Copyright 2025 DeepMind Technologies Limited.**\n",
        "\n",
        "Before diving into this notebook, we highly recommend familiarizing yourself with the [\"Tutorial of DP-SGD LoRA fine-tuning Gemma3 in Keras on SAMSum dataset\"](https://github.com/google-deepmind/jax_privacy/blob/4aa89276aad9d453a9dfb872a672a82396f3a8b7/examples/dp_sgd_keras_gemma3_lora_finetuning_samsum.ipynb) (referred to as the \"DP-SGD LoRA tutorial\" from now on). This tutorial builds upon that previous work, and therefore, code concepts introduced there will not be explained in detail here.\n",
        "\n",
        "This tutorial will guide you through generating **differentially private (DP) synthetic data**. This is a widespread use case, particularly for organizations handling sensitive data who wish to augment it while preserving individual privacy. Generating private synthetic data unlocks many possibilities, such as public data releases, training downstream tasks, or simply replacing sensitive original datasets to mitigate risks.\n",
        "\n",
        "For a more in-depth understanding of synthetic data use cases and related concepts, you can refer to the paper [\"Harnessing large-language models to generate private synthetic text\"](https://arxiv.org/pdf/2306.01684). This tutorial draws inspiration from that paper and adopts a similar experimental setup.\n",
        "\n",
        "In this example, we'll generate synthetic movie reviews that are similar to the [IMDb dataset](http://kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). The overall approach mirrors the DP-SGD LoRA tutorial:\n",
        "\n",
        "1.  We will **DP fine-tune** the [Gemma3 base model](https://www.kaggle.com/models/keras/gemma3) on the IMDb dataset. We'll use a concise prompt: `[imdb][{label}]:`. The model is expected to learn that this prompt signals the task of generating a review aligned with the provided label (positive or negative).\n",
        "2.  Next, we'll **sample** several thousand examples in a \"diverse\" manner using the DP fine-tuned model, producing our DP synthetic data.\n",
        "3.  Finally, we'll **evaluate performance** using the [MAUVE metric](https://github.com/krishnap25/mauve), comparing the generated DP synthetic data with the test portion of the IMDb dataset.\n",
        "\n",
        "This tutorial leverages the **[Keras API](https://jax-privacy.readthedocs.io/en/latest/keras_api.html)** within Jax Privacy for DP model fine-tuning.\n",
        "\n",
        "Performance will be evaluated using the **MAUVE metric**, calculated against real test data across the following setups:\n",
        "\n",
        "1.  **Baseline 1:** Handcrafted prompt without model fine-tuning.\n",
        "2.  **Baseline 2:** Handcrafted prompt with a few examples from the dataset, without model fine-tuning.\n",
        "3.  **DP Fine-tuning:** Model fine-tuned with differential privacy, followed by sampling.\n",
        "4.  **Non-DP Fine-tuning:** Model fine-tuned without differential privacy, followed by sampling.\n",
        "5.  **Real Train Data:** The performance of using the real training data itself, serving as an estimation of a \"good\" result.\n",
        "5.  **Real Test Data:** The performance of using the real test data itself, serving as an ultimate upper bound for achievable results.\n",
        "\n",
        "The precise configurations for each experiment are detailed at the end of this notebook.\n",
        "\n",
        "The results presented in this tutorial were obtained using 4 Cloud TPU v5p devices.\n",
        "\n",
        "A test run of this tutorial (e.g., to verify your environment is correctly configured) can be completed in a few minutes, for example, by using a free v2-8 TPU in Google Colab.\n",
        "\n",
        "\n",
        "<!-- TODO - b/398715962: add resource requirements realistic in OSS -->\n",
        "\n",
        "<!-- TODO - b/398715962: check unfilled links -->"
      ]
    },
    {
      "metadata": {
        "id": "_gl5CllsLKNg"
      },
      "cell_type": "markdown",
      "source": [
        "## Install and import dependencies"
      ]
    },
    {
      "metadata": {
        "id": "RXbMK3ALOFLc"
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3\"\n",
        "!pip uninstall -y -q keras-hub\n",
        "!pip install -q -U keras-hub\n",
        "!pip install tqdm\n",
        "!pip install ipywidgets\n",
        "\n",
        "!pip install dp_accounting jaxtyping drjax\n",
        "!pip install jax_privacy==1.0.0"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "DPjnAVLEQCpE"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_HOME\"] = (\n",
        "    os.getcwd()\n",
        ")  # Ensure that Keras uses home directory, which has enough space\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = (\n",
        "    \"1.00\"  # Avoid memory fragmentation on JAX backend.\n",
        ")\n",
        "\n",
        "import keras\n",
        "import keras_hub\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import json\n",
        "\n",
        "# Jax Privacy deps\n",
        "from jax_privacy.keras import keras_api"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "RyLtHdxVQFif"
      },
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "kagglehub.login()\n",
        "\n",
        "# If you are using Colab, you can alternatively set KAGGLE_USERNAME and KAGGLE_KEY\n",
        "# values in user data, and then uncomment and run the following code:\n",
        "#\n",
        "# from colabtools import userdata\n",
        "#\n",
        "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "# os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "#\n",
        "# You use userdata to keep the Kaggle API key safe. Alternatively, you can\n",
        "# hardcode the values but it is not recommended due to security risks of\n",
        "# leaking the API key.\n",
        "\n",
        "\n",
        "# If you're not using Colab, set the env vars as appropriate for your system.\n",
        "# For example, to set the env vars on Linux you can run in terminal:\n",
        "# ```\n",
        "# export KAGGLE_USERNAME=\"your_username\"\n",
        "# export KAGGLE_KEY=\"your_key\"\n",
        "# ```"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "5Re8i0e3S0wF"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Setup\n",
        "\n",
        "Here, we'll highlight a few important differences from the DP-SGD LoRA tutorial.\n",
        "\n",
        "First, using the default sampler, which always chooses the next token with the highest probability, would result in identical synthetic examples every time we perform inference. To introduce **variability**, we must change the sampler. We achieve this by using a [TopPSampler](https://keras.io/keras_hub/api/samplers/top_p_sampler/) with `TOP_P=0.95`. This means we'll sample from the tokens whose cumulative probability, when ordered by decreasing probability, is at least 0.95. As you'll see, a `TOP_P` value of 0.95 allows for significant diversity in our sampling.\n",
        "\n",
        "We also define the **validation size**, which specifies how many dataset entries are used for validation during training. Note that these entries will differ with each validation step. This number can be quite small, or even zero. While a validation dataset is typically used to monitor model overfitting and enable early stopping, early stopping is less critical when fine-tuning for synthetic data generation. An overfit model can still perform well as a synthetic text generator, and there's generally no issue with that. In fact, if you have ample, non-sensitive data, there's little reason to generate synthetic data; you could simply use the data directly. If your dataset size is insufficient for your goals but the data isn't sensitive, non-DP fine-tuning is an option. However, in many real-world scenarios, data originates from individuals and is thus sensitive, even if not directly identifiable. In such cases, **differentially private fine-tuning** is crucial to ensure the generated synthetic data is also DP.\n",
        "\n",
        "Another set of important parameters relates to the **MAUVE metric**. MAUVE quantifies the similarity between any two text datasets, ranging from 0 to 1. A score of 0 indicates completely different types of text, while 1 suggests the texts are highly similar. When calculating MAUVE, it's crucial for the datasets to contain enough samples; the [original paper](https://arxiv.org/pdf/2102.01454) recommends 5,000 samples as a good balance between speed and quality. To ensure the results aren't specific to a particular sampled dataset, it's advisable to generate more than 5,000 samples (e.g., 10,000) and then calculate MAUVE on multiple (e.g., 5) 5,000-sample subsampled datasets. Subsampling is performed uniformly from the larger dataset. Finally, the mean and standard deviation are calculated across these trials. In our experiment configuration, the following constants control this process: `NUM_SAMPLES` (the larger 10,000-sample dataset), `NUM_MAUVE_SAMPLES` (the size of each subsampled dataset for MAUVE calculation), and `NUM_MAUVE_TRIALS` (the number of times to subsample and calculate MAUVE). These constants apply to both synthetic data and real data.\n",
        "\n",
        "The configuration also specifies the **base prompt** and whether to **fine-tune the model**, as some experiments do not involve fine-tuning and instead use different handcrafted prompts. You can find the specific prompts and other parameters for each experiment at the end of this notebook.\n",
        "\n",
        "Since certain operations, such as generating 10,000 samples, can be time-consuming, we've included helper functions to save intermediate results. These results will be stored in the `SAVE_ROOT_DIR` folder.\n"
      ]
    },
    {
      "metadata": {
        "id": "Q9VHuxm0QH6k"
      },
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    \"GEMMA3_MODEL_TYPE\": \"gemma3_instruct_12b\",\n",
        "    \"SEQUENCE_LENGTH\": 512,\n",
        "    \"EPOCHS\": 3,\n",
        "    \"BATCH_SIZE\": 32,  # Should be a multiple of the number of GPUs available.\n",
        "    \"GRADIENT_ACCUMULATION_STEPS\": (\n",
        "        32\n",
        "    ),  # Effective batch size is BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS, we recommend to make it 1024.\n",
        "    \"LORA_RANK\": 32,\n",
        "    \"LEARNING_RATE\": 0.003,\n",
        "    # Validation size can be small when fine-tuning for synthetic data generation.\n",
        "    \"VALIDATION_SIZE\": 64,\n",
        "    \"SEED\": 42,\n",
        "    # TOP_P is crucial for \"diverse\" sampling, ensuring different results across sampling calls.\n",
        "    \"TOP_P\": 0.95,\n",
        "    # Number of synthetic examples to generate.\n",
        "    \"NUM_SAMPLES\": 10000,\n",
        "    # Batch size for sampling inference.\n",
        "    \"SAMPLE_BATCH_SIZE\": 24,\n",
        "    # Number of samples to use when computing MAUVE.\n",
        "    \"NUM_MAUVE_SAMPLES\": 5000,\n",
        "    # Number of times to calculate MAUVE on subsamples for a more precise estimate (mean and standard deviation).\n",
        "    \"NUM_MAUVE_TRIALS\": 5,\n",
        "    # Whether to use bfloat16 (16-bit float) weights. Not all GPUs support bfloat16 (e.g., V100 does not, A100 does).\n",
        "    \"USE_MIXED_PRECISION\": False,\n",
        "    # The prompt used for synthetic data generation.\n",
        "    \"PROMPT\": \"[imdb][{label}]:\",\n",
        "    # Set to False for experiments that do not require model fine-tuning.\n",
        "    \"FINETUNE_MODEL\": True,\n",
        "    # Set to True for differentially private fine-tuning.\n",
        "    \"USE_DP\": True,\n",
        "    # DP-SGD parameters. Only applicable if USE_DP is True.\n",
        "    \"EPSILON\": 10.0,\n",
        "    \"DELTA\": (\n",
        "        1.5e-5\n",
        "    ),  # Chosen as a value smaller than 1/n^1.1, where n = 25000 (number of training examples).\n",
        "    \"CLIPPING_NORM\": 1.0,\n",
        "    # If TEST_RUN is True, the code will execute on a small subset of data and a smaller model for quick verification.\n",
        "    \"TEST_RUN\": True,\n",
        "    # The root directory for saving all experiment data.\n",
        "    \"SAVE_ROOT_DIR\": \"./experiment_data\",\n",
        "}\n",
        "\n",
        "if CONFIG[\"TEST_RUN\"]:\n",
        "  CONFIG[\"GEMMA3_MODEL_TYPE\"] = \"gemma3_instruct_1b\"\n",
        "  CONFIG[\"SEQUENCE_LENGTH\"] = 128\n",
        "  CONFIG[\"MAX_TRAIN_SIZE\"] = 3000\n",
        "  CONFIG[\"LORA_RANK\"] = 4\n",
        "  CONFIG[\"NUM_SAMPLES\"] = 100\n",
        "  CONFIG[\"SAMPLE_BATCH_SIZE\"] = 8\n",
        "  CONFIG[\"NUM_MAUVE_SAMPLES\"] = 40\n",
        "  CONFIG[\"NUM_MAUVE_TRIALS\"] = 3\n",
        "  CONFIG[\"USE_MIXED_PRECISION\"] = (\n",
        "      False  # The 1b model is small and should fit into most GPUs.\n",
        "  )\n",
        "\n",
        "print(f\"Experiment config:\\n{json.dumps(CONFIG, indent=2)}\")"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "FZGBOq0vkzCw"
      },
      "cell_type": "code",
      "source": [
        "def save_config():\n",
        "  os.makedirs(CONFIG[\"SAVE_ROOT_DIR\"], exist_ok=True)\n",
        "  file_path = os.path.join(CONFIG[\"SAVE_ROOT_DIR\"], \"config.json\")\n",
        "  with open(file_path, \"w\") as f:\n",
        "    # Save as json so it is human-readable and easy to load programmatically.\n",
        "    json.dump(CONFIG, f, indent=2)\n",
        "  print(f\"Saved config to {file_path}\")\n",
        "\n",
        "\n",
        "save_config()"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "F0JJouRVS1BF"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "metadata": {
        "id": "K0SGh9whQORq"
      },
      "cell_type": "code",
      "source": [
        "SOURCE_TRAIN_DS, SOURCE_VALIDATION_DS = tfds.load(\n",
        "    \"imdb_reviews\", split=[\"train\", \"test\"]\n",
        ")\n",
        "\n",
        "if CONFIG[\"TEST_RUN\"]:\n",
        "  print(\"TEST RUN, sampling datasets\")\n",
        "  SOURCE_TRAIN_DS = SOURCE_TRAIN_DS.take(CONFIG[\"MAX_TRAIN_SIZE\"])\n",
        "  SOURCE_VALIDATION_DS = SOURCE_VALIDATION_DS.take(CONFIG[\"MAX_TRAIN_SIZE\"])"
      ],
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "HSj0que-VZ71"
      },
      "cell_type": "markdown",
      "source": [
        "Let's examine an IMDb dataset entry.\n",
        "\n",
        "Each entry is a pair: a **label** (an integer, 1 for positive or 0 for negative) and the **movie review text**."
      ]
    },
    {
      "metadata": {
        "id": "z9J3RXHlQd0P"
      },
      "cell_type": "code",
      "source": [
        "SOURCE_EXAMPLE_DS = SOURCE_VALIDATION_DS.take(1).batch(1, drop_remainder=True)\n",
        "SOURCE_EXAMPLE = SOURCE_EXAMPLE_DS.as_numpy_iterator().next()\n",
        "for key, val in SOURCE_EXAMPLE.items():\n",
        "  if isinstance(val[0], np.int64):\n",
        "    decoded_val = str(val[0])\n",
        "  else:\n",
        "    decoded_val = val[0].decode('utf-8')\n",
        "  print(f'{key}:\\n\"{decoded_val}\"\\n')"
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "8Y3sXJOzViFl"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's transform the dataset entries into model prompts and their corresponding expected responses. For labels, instead of using 1 or 0, our prompts will use the English words \"positive\" or \"negative\". The expected responses will remain the original review texts, without any modifications."
      ]
    },
    {
      "metadata": {
        "id": "gLJ9TjPnQfh9"
      },
      "cell_type": "code",
      "source": [
        "def get_prompt(label_id):\n",
        "  label_str = \"positive\" if label_id == 1 else \"negative\"\n",
        "  return CONFIG[\"PROMPT\"].format(label=label_str)\n",
        "\n",
        "\n",
        "def source_to_gemma3_format(review_dict):\n",
        "  def map_source_example(label, text):\n",
        "    return get_prompt(label), text\n",
        "\n",
        "  prompt, response = tf.py_function(\n",
        "      func=map_source_example,\n",
        "      inp=[review_dict[\"label\"], review_dict[\"text\"]],\n",
        "      Tout=[tf.string, tf.string],\n",
        "  )\n",
        "  prompt.set_shape(())\n",
        "  response.set_shape(())\n",
        "  return {\"prompts\": prompt, \"responses\": response}"
      ],
      "outputs": [],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "ppRlZfC4soLX"
      },
      "cell_type": "code",
      "source": [
        "TRAIN_DS = SOURCE_TRAIN_DS.map(source_to_gemma3_format)\n",
        "VALIDATION_DS = SOURCE_VALIDATION_DS.map(source_to_gemma3_format).take(\n",
        "    CONFIG[\"VALIDATION_SIZE\"]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "cJX2NxcNVrNv"
      },
      "cell_type": "markdown",
      "source": [
        "Let's examine what the model's input will look like. If we are not fine-tuning the model, we won't need `responses`. During inference, we'll provide plain prompts as a list of strings, rather than a dictionary of prompts and responses."
      ]
    },
    {
      "metadata": {
        "id": "o9X6ZrfvRFeb"
      },
      "cell_type": "code",
      "source": [
        "EXAMPLE_DS = VALIDATION_DS.take(1).batch(1, drop_remainder=True)\n",
        "EXAMPLE = EXAMPLE_DS.as_numpy_iterator().next()\n",
        "for key, val in EXAMPLE.items():\n",
        "  decoded_val = val[0].decode('utf-8')\n",
        "  print(f'{key}:\\n\"{decoded_val}\"\\n')"
      ],
      "outputs": [],
      "execution_count": 11
    },
    {
      "metadata": {
        "id": "tVRVpuP_RNmO"
      },
      "cell_type": "code",
      "source": [
        "# Train size is important for DP-SGD.\n",
        "TRAIN_SIZE = int(TRAIN_DS.cardinality().numpy())\n",
        "print(f\"Train size: {TRAIN_SIZE}\")\n",
        "VALIDATION_SIZE = int(VALIDATION_DS.cardinality().numpy())\n",
        "print(f\"Validation size: {VALIDATION_SIZE}\")\n",
        "\n",
        "TRAIN_DS = TRAIN_DS.shuffle(buffer_size=2048).batch(\n",
        "    CONFIG[\"BATCH_SIZE\"], drop_remainder=True\n",
        ")\n",
        "VALIDATION_DS = VALIDATION_DS.batch(CONFIG[\"BATCH_SIZE\"], drop_remainder=True)"
      ],
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "id": "4nb7uZjcR7XX"
      },
      "cell_type": "code",
      "source": [
        "if len(jax.devices()) > 1:\n",
        "  DATA_PARALLEL = keras.distribution.DataParallel()\n",
        "  # You can see over how many GPUs the data will be distributed.\n",
        "  print(DATA_PARALLEL)\n",
        "  keras.distribution.set_distribution(DATA_PARALLEL)\n",
        "else:\n",
        "  print(\"Only one device, there will be no data parallelism\")"
      ],
      "outputs": [],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "8lrld43OZPdh"
      },
      "cell_type": "markdown",
      "source": [
        "## Model setup and fine-tuning"
      ]
    },
    {
      "metadata": {
        "id": "kz5eUq2mSAZu"
      },
      "cell_type": "code",
      "source": [
        "MODEL_WEIGHTS_DTYPE = None  # use default dtype\n",
        "if CONFIG[\"USE_MIXED_PRECISION\"]:\n",
        "  print(\"Using mixed precision\")\n",
        "  keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
        "  MODEL_WEIGHTS_DTYPE = \"bfloat16\"\n",
        "\n",
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\n",
        "    CONFIG[\"GEMMA3_MODEL_TYPE\"], dtype=MODEL_WEIGHTS_DTYPE\n",
        ")\n",
        "\n",
        "assert isinstance(\n",
        "    gemma_lm.preprocessor, keras_hub.models.Gemma3CausalLMPreprocessor\n",
        ")\n",
        "gemma_lm.preprocessor.sequence_length = CONFIG[\"SEQUENCE_LENGTH\"]\n",
        "gemma_lm.summary()"
      ],
      "outputs": [],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "_tXrOwbTSEIT"
      },
      "cell_type": "code",
      "source": [
        "if CONFIG[\"FINETUNE_MODEL\"]:\n",
        "  gemma_lm.backbone.enable_lora(rank=CONFIG[\"LORA_RANK\"])\n",
        "  gemma_lm.summary()\n",
        "else:\n",
        "  print(\"Not finetuning model\")"
      ],
      "outputs": [],
      "execution_count": 15
    },
    {
      "metadata": {
        "id": "qTiHoVQ06J_F"
      },
      "cell_type": "code",
      "source": [
        "def load_lora_weights(filename: str):\n",
        "  filepath = os.path.join(CONFIG[\"SAVE_ROOT_DIR\"], filename)\n",
        "  gemma_lm.backbone.load_lora_weights(filepath)\n",
        "  print(f\"LoRA weights loaded from: {filepath}\")\n",
        "\n",
        "\n",
        "# Uncomment it if you've already finetuned the model and want to load the weights.\n",
        "# load_lora_weights(\"weights.lora.h5\")"
      ],
      "outputs": [],
      "execution_count": 16
    },
    {
      "metadata": {
        "id": "OZCMAKkkV2Ko"
      },
      "cell_type": "markdown",
      "source": [
        "### Enabling Differentially Private Fine-tuning\n",
        "\n",
        "The pre-trained model itself is not differentially private with respect to its initial training data, which we consider non-sensitive. However, the data used for fine-tuning in real world scenarioes usually *is* sensitive. To ensure the privacy of this sensitive data (IMDb reviews in our case), we prepare the model so that any subsequent training on this data will be differentially private. This process makes our fine-tuned model differentially private with respect to the sensitive fine-tuning data."
      ]
    },
    {
      "metadata": {
        "id": "OniunY4ISOa1"
      },
      "cell_type": "code",
      "source": [
        "if CONFIG[\"USE_DP\"]:\n",
        "  params = keras_api.DPKerasConfig(\n",
        "      epsilon=CONFIG[\"EPSILON\"],\n",
        "      delta=CONFIG[\"DELTA\"],\n",
        "      clipping_norm=CONFIG[\"CLIPPING_NORM\"],\n",
        "      batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "      train_steps=CONFIG[\"EPOCHS\"] * (TRAIN_SIZE // CONFIG[\"BATCH_SIZE\"]),\n",
        "      train_size=TRAIN_SIZE,\n",
        "      gradient_accumulation_steps=CONFIG[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
        "      seed=CONFIG[\"SEED\"],\n",
        "  )\n",
        "  gemma_lm = keras_api.make_private(gemma_lm, params)\n",
        "  print(\n",
        "      \"DP training:\"\n",
        "      f\"{CONFIG['CLIPPING_NORM']=} {CONFIG['EPOCHS']=} {CONFIG['BATCH_SIZE']=}\"\n",
        "  )\n",
        "else:\n",
        "  print(\"Non-DP training\")"
      ],
      "outputs": [],
      "execution_count": 17
    },
    {
      "metadata": {
        "id": "opThz2VVtsE1"
      },
      "cell_type": "code",
      "source": [
        "if CONFIG[\"FINETUNE_MODEL\"]:\n",
        "  optimizer = keras.optimizers.Adam(\n",
        "      learning_rate=CONFIG[\"LEARNING_RATE\"],\n",
        "      gradient_accumulation_steps=CONFIG[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
        "  )\n",
        "\n",
        "  gemma_lm.compile(\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      optimizer=optimizer,\n",
        "      weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "  )\n",
        "else:\n",
        "  print(\"Not finetuning model\")"
      ],
      "outputs": [],
      "execution_count": 18
    },
    {
      "metadata": {
        "id": "UgUD1EW2reSo"
      },
      "cell_type": "code",
      "source": [
        "def save_lora_weights(filename: str):\n",
        "  filepath = os.path.join(CONFIG[\"SAVE_ROOT_DIR\"], filename)\n",
        "  gemma_lm.backbone.save_lora_weights(filepath)\n",
        "  print(f\"LoRA weights saved to: {filepath}\")"
      ],
      "outputs": [],
      "execution_count": 19
    },
    {
      "metadata": {
        "id": "25lpUo-2aWn1"
      },
      "cell_type": "markdown",
      "source": [
        "### Do LoRA fine-tuning"
      ]
    },
    {
      "metadata": {
        "id": "pn6y66E5SPEU"
      },
      "cell_type": "code",
      "source": [
        "if CONFIG[\"FINETUNE_MODEL\"]:\n",
        "  gemma_lm.fit(\n",
        "      x=TRAIN_DS, epochs=CONFIG[\"EPOCHS\"], validation_data=VALIDATION_DS\n",
        "  )\n",
        "  save_lora_weights(\"weights.lora.h5\")\n",
        "else:\n",
        "  print(\"Not finetuning model\")"
      ],
      "outputs": [],
      "execution_count": 20
    },
    {
      "metadata": {
        "id": "xKGNhpKHWPDr"
      },
      "cell_type": "markdown",
      "source": [
        "## Synthetic Data Generation\n",
        "\n",
        "In this section, we will generate synthetic data using the model we've loaded and, optionally, fine-tuned.\n",
        "\n",
        "Once you have the model, the synthetic data generation process is straightforward: we simply provide prompts (omitting expected responses since we're no longer training) and instruct the model to complete them (i.e., perform standard inference). These prompts can be either a short task-specific prompt, such as `[imdb][<positive/negative>]:` (if the model has been fine-tuned), or a longer, human-readable prompt explaining the task to the model, potentially including examples from the real dataset if fine-tuning has not occurred. It's important to note that if you include examples from the original dataset, your generated data is, strictly speaking, no longer differentially private. In such cases, a minimum safeguard is to manually review the included examples.\n",
        "\n",
        "First, as discussed earlier, we must update the model's sampler to ensure it produces different results with each inference."
      ]
    },
    {
      "metadata": {
        "id": "nulCM_o-S2dm"
      },
      "cell_type": "code",
      "source": [
        "sampler = keras_hub.samplers.TopPSampler(p=CONFIG[\"TOP_P\"])\n",
        "gemma_lm.compile(sampler=sampler)"
      ],
      "outputs": [],
      "execution_count": 21
    },
    {
      "metadata": {
        "id": "jhvnCyfAW5B4"
      },
      "cell_type": "markdown",
      "source": [
        "Let's manually examine some generated examples. If the model hasn't been fine-tuned, the examples might appear \"okayish,\" but with fine-tuning, they will more closely resemble real data."
      ]
    },
    {
      "metadata": {
        "id": "R45RdKb5iCfp"
      },
      "cell_type": "code",
      "source": [
        "def generate_n_examples(n: int, is_positive: bool) -> list[str]:\n",
        "  label = 1 if is_positive else 0\n",
        "  return [gemma_lm.generate(get_prompt(label)) for _ in range(n)]\n",
        "\n",
        "\n",
        "def print_generated_texts(texts: list[str], label: str):\n",
        "  examples_to_print = \"\\n------\\n\".join(texts)\n",
        "  print(f\"Generated {label} examples:\\n{examples_to_print}\")\n",
        "\n",
        "\n",
        "print_generated_texts(generate_n_examples(n=3, is_positive=True), \"positive\")\n",
        "print(\"\\n=========\\n\")\n",
        "print_generated_texts(generate_n_examples(n=3, is_positive=False), \"negative\")"
      ],
      "outputs": [],
      "execution_count": 22
    },
    {
      "metadata": {
        "id": "V35Myy4CW_vT"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's generate the synthetic data! We will produce it in the same format as the input dataset: a list of Python dictionaries, each containing `{\"label\": <value>, \"review\": <value>}`. Since generating thousands of samples can take several hours, it's a good practice to save the data immediately to prevent loss.\n",
        "\n",
        "**Note**: Generation can take a significant amount of time if you have a large model, long sequence length, small physical batch size, or a small number of GPUs for data parallelization. Adjust the parameters accordingly to speed up generation."
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "0LOEevoflpjY"
      },
      "cell_type": "code",
      "source": [
        "# @title Synthetic Data Generation Functions\n",
        "\n",
        "\n",
        "def extract_label_and_review(responses, prompts_by_label):\n",
        "  cleaned_texts = []\n",
        "  for text in responses:\n",
        "    for label, prompt in prompts_by_label.items():\n",
        "      if text.startswith(prompt):\n",
        "        text = text[len(prompt) :].replace(\"<end_of_turn>\", \"\")\n",
        "        cleaned_texts.append({\"label\": label, \"text\": text})\n",
        "        break\n",
        "  return cleaned_texts\n",
        "\n",
        "\n",
        "def generate_synthetic_data(num_samples, sample_batch_size, seed):\n",
        "  base_prompts = {\n",
        "      0: get_prompt(0),  # negative\n",
        "      1: get_prompt(1),  # positive\n",
        "  }\n",
        "  prompt_rng = jax.random.key(seed)\n",
        "  prompt_arange = jnp.arange(len(base_prompts))\n",
        "  prompt_indices = jax.random.choice(\n",
        "      prompt_rng, prompt_arange, shape=(num_samples,)\n",
        "  )\n",
        "  prompt_splits = jnp.array_split(\n",
        "      prompt_indices, num_samples // sample_batch_size\n",
        "  )\n",
        "  pbar = tqdm.tqdm(total=num_samples)\n",
        "  result = []\n",
        "  for local_indices in prompt_splits:\n",
        "    prompts = [base_prompts[int(i.item())] for i in local_indices]\n",
        "    responses = gemma_lm.generate(prompts)\n",
        "    label_and_reviews = extract_label_and_review(responses, base_prompts)\n",
        "    pbar.update(sample_batch_size)\n",
        "    result += label_and_reviews\n",
        "  return result\n",
        "\n",
        "\n",
        "def save_data(data, filename):\n",
        "  os.makedirs(CONFIG[\"SAVE_ROOT_DIR\"], exist_ok=True)\n",
        "  file_path = os.path.join(CONFIG[\"SAVE_ROOT_DIR\"], filename)\n",
        "  with open(file_path, \"w\") as f:\n",
        "    json.dump(data, f, indent=2)\n",
        "  print(f\"Saved data to {file_path}\")\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "  file_path = os.path.join(CONFIG[\"SAVE_ROOT_DIR\"], filename)\n",
        "  with open(file_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "  print(f\"Loaded data from {file_path}\")\n",
        "  return data"
      ],
      "outputs": [],
      "execution_count": 23
    },
    {
      "metadata": {
        "id": "1su09lR1mgbv"
      },
      "cell_type": "code",
      "source": [
        "SYNTHETIC_DATA = generate_synthetic_data(\n",
        "    CONFIG[\"NUM_SAMPLES\"], CONFIG[\"SAMPLE_BATCH_SIZE\"], CONFIG[\"SEED\"]\n",
        ")\n",
        "save_data(SYNTHETIC_DATA, \"synthetic_data.json\")\n",
        "# Comment two previous lines and uncomment the next line to load the already generated data.\n",
        "# SYNTHETIC_DATA = load_data(\"synthetic_data.json\")\n",
        "print(f\"\\nGenerated {len(SYNTHETIC_DATA)} synthetic examples.\")\n",
        "print(f\"First synthetic example:\\n{SYNTHETIC_DATA[0]}\")\n",
        "SYNTHETIC_DATA_TEXTS = [example[\"text\"] for example in SYNTHETIC_DATA]"
      ],
      "outputs": [],
      "execution_count": 24
    },
    {
      "metadata": {
        "id": "8Mq3HeEhXVmZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "As previously mentioned, we evaluate the quality of the synthetic data using the **MAUVE metric**.\n",
        "\n",
        "Unfortunately, at the time of writing this tutorial, the scalable library we used for MAUVE calculation is not open-sourced. Therefore, we cannot provide code for directly calculating MAUVE here. We plan to implement an alternative solution using open-source libraries soon, and this notebook will be updated accordingly.\n",
        "\n",
        "For now, we will explain how you can perform this calculation using available open-source libraries and present the results we obtained.\n",
        "\n",
        "You can use the official [Mauve library](https://github.com/krishnap25/mauve) from GitHub. Its interface is quite straightforward. The primary limitation is that it's not highly parallelizable, as it runs on a single GPU, making text-to-embedding conversion a bottleneck. To overcome this, we recommend calculating the text embeddings in parallel independently and then providing these pre-computed embeddings to the MAUVE library. This approach significantly speeds up score calculation. Keep in mind that our methodology involves subsampling and multiple MAUVE calculations; this logic needs to be implemented manually as it's not part of the library. The subsampling strategy was described earlier: generate 10,000 samples, then uniformly subsample 5,000 samples five times for both synthetic and real data, and calculate the MAUVE score for each subsample.\n",
        "\n",
        "The method for calculating embeddings is crucial. In our evaluation, we used the [Gecko English model](https://arxiv.org/pdf/2403.20327), which has 110 million parameters and produces 768-dimensional embedding vectors. While the Gecko model weights are not publicly released, you can perform inference using Vertex AI on Google Cloud (refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api) and this [notebook](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/text_embedding_new_api.ipynb)). If you aim to reproduce our results, use `text-embedding-005`, which is the most similar model available.\n",
        "\n",
        "Alternatively, you can create text embeddings by using any other general-purpose LLM and extracting its last hidden layer. For instance, you could take the base, non-instruct Gemma3 model and use the last hidden layer of its last non-padding token. The advantage of this approach is that you can utilize the same resources running this notebook to their maximum capacity, such as computing embeddings for multiple texts in parallel."
      ]
    },
    {
      "metadata": {
        "id": "b_GjjkjIYNLf"
      },
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "\n",
        "For Gemma3 12b (`gemma3_instruct_12b`), fine-tuned for instructions, you can expect the following MAUVE scores:\n",
        "\n",
        "| Experiment | MAUVE |\n",
        "|---|---|\n",
        "| `baseline_without_examples`: Baseline (no fine-tuning, no examples) | 0.006 ± 0.000 |\n",
        "| `baseline_with_examples`: Baseline (no fine-tuning, 6 examples) | 0.008 ± 0.000 |\n",
        "| `dp_ft`: DP Synthetic Data | 0.795 ± 0.010 |\n",
        "| `non_dp_ft`: Non-DP Synthetic Data | 0.856 ± 0.008 |\n",
        "| `use_train`: Train real vs. test real data | 0.806 ± 0.013 |\n",
        "| `upper_bound`: Test real vs. test real data | 0.964 ± 0.003 |\n",
        "\n",
        "As anticipated, the results align with our expectations. The baselines exhibit very poor scores because the model lacks knowledge of the desired format and style for generating reviews. DP fine-tuning performs slightly worse than non-DP fine-tuning, but the difference is not dramatic. Both fine-tuning approaches yield lower scores than simply comparing two samples from the test data (`upper_bound`), which represents the theoretical best performance achievable in synthetic text data generation.\n",
        "\n",
        "One potentially surprising observation is that using the training data directly yields roughly similar results to DP synthetic data, yet worse results than non-DP synthetic data. This phenomenon is explained by how the dataset was split into training and testing sets, as detailed in the [original paper](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf). The train and test splits do not overlap in terms of specific movies, leading to slightly different linguistic patterns. The fact that synthetic data generated by a fine-tuned model is more similar to the test dataset can be attributed to the LLM learning to produce \"generic\" movie reviews that capture the essential characteristics of a movie review, irrespective of the particular film. This generalized understanding allows its output to statistically align more closely with the test set.\n",
        "\n",
        "Below, you'll find the detailed configurations for each experiment.\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary><b>baseline_without_examples</b></summary>\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"GEMMA3_MODEL_TYPE\": \"gemma3_instruct_12b\",\n",
        "  \"SEQUENCE_LENGTH\": 512,\n",
        "  \"EPOCHS\": 3,\n",
        "  \"BATCH_SIZE\": 32,\n",
        "  \"GRADIENT_ACCUMULATION_STEPS\": 32,\n",
        "  \"LORA_RANK\": 32,\n",
        "  \"LEARNING_RATE\": 0.003,\n",
        "  \"VALIDATION_SIZE\": 64,\n",
        "  \"SEED\": 42,\n",
        "  \"TOP_P\": 0.95,\n",
        "  \"NUM_SAMPLES\": 10000,\n",
        "  \"SAMPLE_BATCH_SIZE\": 24,\n",
        "  \"NUM_MAUVE_SAMPLES\": 5000,\n",
        "  \"NUM_MAUVE_TRIALS\": 5,\n",
        "  \"USE_MIXED_PRECISION\": false,\n",
        "  \"FINETUNE_MODEL\": false,\n",
        "  \"PROMPT\": \"As an IMDb movie reviewer, generate a realistic, one paragraph {label} movie review. Make up concrete names for characters or other details in your review. Just provide the review text itself, no ratings, sections, or extra info.\",\n",
        "  \"USE_DP\": false,\n",
        "  \"EPSILON\": 10.0,\n",
        "  \"DELTA\": 1.5e-05,\n",
        "  \"CLIPPING_NORM\": 1.0,\n",
        "  \"TEST_RUN\": false\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><b>baseline_with_examples</b></summary>\n",
        "\n",
        "Since we are inserting examples, the prompt becomes significantly larger (~500 tokens). For proper evaluation, we must increase the maximum sequence length to provide enough space for the model to generate its prediction.\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"GEMMA3_MODEL_TYPE\": \"gemma3_instruct_12b\",\n",
        "  \"SEQUENCE_LENGTH\": 1024,\n",
        "  \"EPOCHS\": 3,\n",
        "  \"BATCH_SIZE\": 16,\n",
        "  \"GRADIENT_ACCUMULATION_STEPS\": 64,\n",
        "  \"LORA_RANK\": 32,\n",
        "  \"LEARNING_RATE\": 0.003,\n",
        "  \"VALIDATION_SIZE\": 64,\n",
        "  \"SEED\": 42,\n",
        "  \"TOP_P\": 0.95,\n",
        "  \"NUM_SAMPLES\": 10000,\n",
        "  \"SAMPLE_BATCH_SIZE\": 16,\n",
        "  \"NUM_MAUVE_SAMPLES\": 5000,\n",
        "  \"NUM_MAUVE_TRIALS\": 5,\n",
        "  \"USE_MIXED_PRECISION\": false,\n",
        "  \"FINETUNE_MODEL\": false,\n",
        "  \"PROMPT\": \"As an IMDb movie reviewer, generate a realistic, one paragraph movie review in the same format and similar style as the examples below. Make up concrete names for characters or other details in your review. Only provide the review text.\\n\\nHere are three examples of positive movie reviews:\\n\\n* `This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.`\\n* `I just saw the movie on tv. I really enjoyed it. I like a good mystery. and this one had me guessing up to the end. Sean Connery did a good job. I would recomend it to a friend.`\\n* `I really enjoyed this movie.I was fifteen when this movie came out and I could relate. This will be a movie I would show my kids to let them know, the feelings they are having are normal. It is funny to see how we could be so devestated by things at such a young age..who knew that we would bounce back....again and again....Great movie!!!!`\\n\\nHere are three examples of negative movie reviews:\\n\\n* `It was disgusting and painful. What a waste of a cast! I swear, the audience (1/2 full) laughed TWICE in 90 minutes. This is not a lie. Do not even rent it.<br /><br />Zeta Jones was just too mean to be believable.<br /><br />Cusack was OK. Just OK. I felt sorry for him (the actor) in case people remember this mess.<br /><br />Roberts was the same as she always is. Charming and sweet, but with no purpose. The \\\"romance\\\" with John was completely unbelievable.`\\n* `I'm sorry, I had high hopes for this movie. Unfortunately, it was too long, too thin and too weak to hold my attention. When I realized the whole movie was indeed only about an older guy reliving his dream, I felt cheated. Surely it could have been a device to bring us into something deeper, something more meaningful.<br /><br />So, don't buy a large drink or you'll be running to the rest room. My kids didn't enjoy it either. Ah well.`\\n* `Corky Romano has to be one of the most jaw dropping and horrific \\\"comedy's\\\" ever made.<br /><br />While the sometimes amusing Chris Kattan who pulled off a very funny performance in the hilarious 'Undercover Brother' his character in Corky is so stupid and so unfunny-which is a shame since the premise is a wonderful idea. To bad they ran out of them when they got to page 3 on the script.`\\n\\nNow it's your turn, generate one {label} movie review which is similar to the {label} examples above:\",\n",
        "  \"USE_DP\": false,\n",
        "  \"EPSILON\": 10.0,\n",
        "  \"DELTA\": 1.5e-05,\n",
        "  \"CLIPPING_NORM\": 1.0,\n",
        "  \"TEST_RUN\": false\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><b>dp_ft</b></summary>\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"GEMMA3_MODEL_TYPE\": \"gemma3_instruct_12b\",\n",
        "  \"SEQUENCE_LENGTH\": 512,\n",
        "  \"EPOCHS\": 3,\n",
        "  \"BATCH_SIZE\": 32,\n",
        "  \"GRADIENT_ACCUMULATION_STEPS\": 32,\n",
        "  \"LORA_RANK\": 32,\n",
        "  \"LEARNING_RATE\": 0.003,\n",
        "  \"VALIDATION_SIZE\": 64,\n",
        "  \"SEED\": 42,\n",
        "  \"TOP_P\": 0.95,\n",
        "  \"NUM_SAMPLES\": 10000,\n",
        "  \"SAMPLE_BATCH_SIZE\": 24,\n",
        "  \"NUM_MAUVE_SAMPLES\": 5000,\n",
        "  \"NUM_MAUVE_TRIALS\": 5,\n",
        "  \"USE_MIXED_PRECISION\": false,\n",
        "  \"FINETUNE_MODEL\": true,\n",
        "  \"PROMPT\": \"[imdb][{label}]:\",\n",
        "  \"USE_DP\": true,\n",
        "  \"EPSILON\": 10.0,\n",
        "  \"DELTA\": 1.5e-05,\n",
        "  \"CLIPPING_NORM\": 1.0,\n",
        "  \"TEST_RUN\": false\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><b>non_dp_ft</b></summary>\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"GEMMA3_MODEL_TYPE\": \"gemma3_instruct_12b\",\n",
        "  \"SEQUENCE_LENGTH\": 512,\n",
        "  \"EPOCHS\": 3,\n",
        "  \"BATCH_SIZE\": 32,\n",
        "  \"GRADIENT_ACCUMULATION_STEPS\": 32,\n",
        "  \"LORA_RANK\": 32,\n",
        "  \"LEARNING_RATE\": 0.003,\n",
        "  \"VALIDATION_SIZE\": 64,\n",
        "  \"SEED\": 42,\n",
        "  \"TOP_P\": 0.95,\n",
        "  \"NUM_SAMPLES\": 10000,\n",
        "  \"SAMPLE_BATCH_SIZE\": 24,\n",
        "  \"NUM_MAUVE_SAMPLES\": 5000,\n",
        "  \"NUM_MAUVE_TRIALS\": 5,\n",
        "  \"USE_MIXED_PRECISION\": false,\n",
        "  \"FINETUNE_MODEL\": true,\n",
        "  \"PROMPT\": \"[imdb][{label}]:\",\n",
        "  \"USE_DP\": false,\n",
        "  \"EPSILON\": 10.0,\n",
        "  \"DELTA\": 1.5e-05,\n",
        "  \"CLIPPING_NORM\": 1.0,\n",
        "  \"TEST_RUN\": false\n",
        "}\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><b>use_train</b></summary>\n",
        "\n",
        "No specific configuration is provided here, as this simply involves calculating the MAUVE score between samples from the train and test datasets.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><b>upper_bound</b></summary>\n",
        "\n",
        "No specific configuration is provided here, as this simply involves calculating the MAUVE score between two **different** samples from the test dataset.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "metadata": {
        "id": "eabVTLfyQGnz"
      },
      "cell_type": "markdown",
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//research/colab/notebook:notebook_backend_ephemeral",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
