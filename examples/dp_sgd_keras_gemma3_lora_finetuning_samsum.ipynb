{
  "cells": [
    {
      "metadata": {
        "id": "6bbWDe0XvDF4"
      },
      "cell_type": "markdown",
      "source": [
        "<div style=\"margin-bottom: 1em;\">\n",
        "  <a href=\"https://colab.research.google.com/github/google-deepmind/jax_privacy/blob/main/examples/dp_sgd_keras_gemma3_lora_finetuning_samsum.ipynb\" target=\"_blank\">\n",
        "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" />\n",
        "  </a>\n",
        "  <a href=\"https://github.com/google-deepmind/jax_privacy/blob/main/examples/dp_sgd_keras_gemma3_lora_finetuning_samsum.ipynb\" target=\"_blank\" style=\"margin-left: 10px;\">\n",
        "    <img src=\"https://img.shields.io/badge/GitHub-view--source-black?logo=github\" />\n",
        "  </a>\n",
        "</div>"
      ]
    },
    {
      "metadata": {
        "id": "TIqLHAS4AXQK"
      },
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "dgHOASupBK6m"
      },
      "cell_type": "markdown",
      "source": [
        "# Tutorial of DP-SGD LoRA fine-tuning Gemma3 in Keras on SAMSum dataset\n",
        "\n",
        "**Copyright 2025 DeepMind Technologies Limited.**\n",
        "\n",
        "Welcome to Jax Privacy for Keras! In this tutorial you will learn how to LoRA fine-tune [Gemma3 LLM](https://www.kaggle.com/models/keras/gemma3) in a differentially private (DP) way using [DP-SGD algorithm](https://medium.com/pytorch/differential-privacy-series-part-1-dp-sgd-algorithm-explained-12512c3959a3). We will fine-tune the model on the [SAMSum dataset](https://huggingface.co/datasets/Samsung/samsum).\n",
        "\n",
        "To perform the full fine-tuning and reproduce the results we recommend using A100 GPU, ideally multiple (e.g. 8 or 16), to speed up the training process. You can obtain them in [Google Colab](https://colab.research.google.com/) or in [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai). For Gemma3 4b, the model we fine-tune in this tutorial, you need at least [16GB of memory in GPU](https://ai.google.dev/gemma/docs/core#sizes). If you don't have it you can use a smaller Gemma 1B model or enable mixed precision (see below).\n",
        "\n",
        "The following links might be helpful as complementary material:\n",
        "\n",
        "* [Gemma3 Overview](https://ai.google.dev/gemma/docs/core#sizes): Good introduction explaining what Gemma3 is.\n",
        "* [Fine-tune Gemma in Keras using LoRA](https://ai.google.dev/gemma/docs/core/lora_tuning): Gemma3 fine-tuning without DP and on a different dataset, our notebook is very similar to this one.\n",
        "* [KerasHub: Get started with Gemma 3](https://www.kaggle.com/code/abheesht75/kerashub-get-started-with-gemma-3): KerasHub tutorial how to make predictions with Gemma3 model (including images).\n",
        "* [Distributed tuning with Gemma using Keras](https://ai.google.dev/gemma/docs/core/distributed_tuning): Gemma3 fine-tuning with model distribution, useful if you want to fine-tune Gemma3 12B or 27B versions. In our example we do only data distribution, model is not distributed."
      ]
    },
    {
      "metadata": {
        "id": "jcJSyOzyGFaA"
      },
      "cell_type": "markdown",
      "source": [
        "## Install and import dependencies"
      ]
    },
    {
      "metadata": {
        "id": "Ylj4Uj_m7Tjd",
        "language": "bash"
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U \"keras>=3\"\n",
        "!pip uninstall -y -q keras-hub\n",
        "!pip install -q -U keras-hub\n",
        "!pip install rouge-score\n",
        "!pip install tqdm\n",
        "!pip install ipywidgets\n",
        "\n",
        "!pip install dp_accounting jaxtyping drjax\n",
        "!pip install jax_privacy==1.0.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ID56z3Wq7Tjd"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_HOME\"] = (\n",
        "    os.getcwd()\n",
        ")  # Ensure that Keras uses home directory, which has enough space\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = (\n",
        "    \"1.00\"  # Avoid memory fragmentation on JAX backend.\n",
        ")\n",
        "\n",
        "import keras\n",
        "import keras_hub\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "\n",
        "# Jax Privacy deps\n",
        "from jax_privacy.keras import keras_api"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "tDBqj4rD7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "## Login to Kaggle\n",
        "\n",
        "It is necessary to download the Gemma3 model. You might also have to give some consents.\n",
        "\n",
        "See more information [here](https://ai.google.dev/gemma/docs/core/distributed_tuning#kaggle_credentials)."
      ]
    },
    {
      "metadata": {
        "id": "rDBGyEwW7Tje"
      },
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "kagglehub.login()\n",
        "\n",
        "# If you are using Colab, you can alternatively set KAGGLE_USERNAME and KAGGLE_KEY\n",
        "# values in user data, and then uncomment and run the following code:\n",
        "#\n",
        "# from colabtools import userdata\n",
        "#\n",
        "# os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "# os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "#\n",
        "# You use userdata to keep the Kaggle API key safe. Alternatively, you can\n",
        "# hardcode the values but it is not recommended due to security risks of\n",
        "# leaking the API key.\n",
        "\n",
        "\n",
        "# If you're not using Colab, set the env vars as appropriate for your system.\n",
        "# For example, to set the env vars on Linux you can run in terminal:\n",
        "# ```\n",
        "# export KAGGLE_USERNAME=\"your_username\"\n",
        "# export KAGGLE_KEY=\"your_key\"\n",
        "# ```"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "dHnLmu0W7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameters setup\n",
        "\n",
        "We are going to fine-tune Gemma3 model with 4 billion parameters with full 32-bit weights. Such a Gemma3 model is 16GB and can fit into A100 GPU with 40GB. [Here](https://ai.google.dev/gemma/docs/core#sizes) you can find other model options and their memory requirements. For example, you can fit 16 bit Gemma3 12B into 40 GB A100 GPU.\n",
        "\n",
        "The maximum sequence length in tokens will be 512. It is enough for the SAMSum dataset, 95% of examples have length smaller than that. Making the length too large will make inference slower.\n",
        "\n",
        "The batch size should be multiple to the number of GPUs you are going to use. This is because we are going to do data parallelism (distribution): batch examples will be evenly distributed over all available GPUs and they will be processed in parallel. We don't do model parallelism (i.e. split the model weights on multiple GPUs), however if you are going to use a larger model (e.g. 27B) you will likely need to distribute it.\n",
        "\n",
        "Gradient accumulation is an important step for DP training: DP works better the larger the effective batch size is. Effective batch size is the total number of examples the model processes before making a step in optimizer, it is equal to `gradient_accumulation_steps * batch_size`. We can't make the physical batch size that large because it won't fit into the memory. You can make test batch size bigger (usually twice larger) to speed up the evaluation.\n",
        "\n",
        "You can set `USE_MIXED_PRECISION` to True if your model does not fit into GPU memory. This will be the case, for example, for A100 40 GB GPU and Gemma3 12B model. Note that the results in this notebook were obtained without mixed precision and the given hyper parameters might not work good with the mixed precision training, so you might need to adjust them.\n",
        "\n",
        "By switching `USE_DP` on or off you can play around and compare DP and non-DP fine-tuning.\n",
        "\n",
        "Epsilon, delta and clipping norm are the main DP parameters. You can read about their meaning in DP literature, e.g. [here](https://medium.com/pytorch/differential-privacy-series-part-1-dp-sgd-algorithm-explained-12512c3959a3).\n",
        "\n",
        "Set `TEST_RUN` to True if you don't have a high-performance GPU right now but still want to run the whole notebook to check that it works for you and play with it a little bit. However, you won't be able to fully fine-tune the model."
      ]
    },
    {
      "metadata": {
        "id": "QUh34l6M7Tje"
      },
      "cell_type": "code",
      "source": [
        "GEMMA3_MODEL_TYPE = \"gemma3_instruct_4b_text\"\n",
        "SEQUENCE_LENGTH = 512\n",
        "TEST_DS_SEQUENCE_LENGTH = 512\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16  # Should be multiple of the number of GPUs you have.\n",
        "GRADIANT_ACCUMULATION_STEPS = 64  # i.e. effective batch size is 16 * 64 = 1024\n",
        "TEST_DS_BATCH_SIZE = 16\n",
        "LORA_RANK = 32\n",
        "LEARNING_RATE = 0.003\n",
        "SEED = 0\n",
        "\n",
        "# Use bfloat16 (i.e. 16-bit float) weights or not. Not all GPUs support bfloat16 (e.g. V100 does not support it, A100 does).\n",
        "USE_MIXED_PRECISION = False\n",
        "\n",
        "USE_DP = True\n",
        "# DP-SGD parameters.\n",
        "EPSILON = 4.0\n",
        "DELTA = 2e-5  # chosen as a value smaller than 1/n^1.1 ~ 2.6e-5 where n = 14732 is number of examples in the training set.\n",
        "CLIPPING_NORM = 0.001\n",
        "\n",
        "# TEST_RUN executes on small data and small model, useful just to check that\n",
        "# the code runs successfully in your environment.\n",
        "TEST_RUN = False\n",
        "if TEST_RUN:\n",
        "  GEMMA3_MODEL_TYPE = \"gemma3_instruct_1b\"\n",
        "  SEQUENCE_LENGTH = 256\n",
        "  TEST_DS_SEQUENCE_LENGTH = 256\n",
        "  MAX_TRAIN_SIZE = 3000\n",
        "  LORA_RANK = 4\n",
        "  USE_MIXED_PRECISION = (\n",
        "      False  # 1b model is small and likely to fit into any GPU.\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "zDxJ7E-p7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "## Training data"
      ]
    },
    {
      "metadata": {
        "id": "FrMhtJps7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the train and validation datasets\n",
        "\n",
        "Each example in the SAMSum dataset is a triple: example id, dialogue and its summary."
      ]
    },
    {
      "metadata": {
        "id": "O_2F7LGF7Tje"
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "manual_dir = \"/root/tensorflow_datasets/downloads/manual\"\n",
        "os.makedirs(manual_dir, exist_ok=True)\n",
        "\n",
        "!wget -O corpus.7z https://arxiv.org/src/1911.12237v2/anc/corpus.7z\n",
        "!sudo apt-get install -y p7zip-full\n",
        "!7z x corpus.7z -ocorpus_extracted -y\n",
        "!mv corpus_extracted/* /root/tensorflow_datasets/downloads/manual"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "xbiOzP8s7Tje"
      },
      "cell_type": "code",
      "source": [
        "SOURCE_TRAIN_DS, SOURCE_VALIDATION_DS = tfds.load(\n",
        "    'samsum', split=['train', 'validation']\n",
        ")\n",
        "\n",
        "if TEST_RUN:\n",
        "  SOURCE_TRAIN_DS = SOURCE_TRAIN_DS.take(MAX_TRAIN_SIZE)\n",
        "  SOURCE_VALIDATION_DS = SOURCE_VALIDATION_DS.take(MAX_TRAIN_SIZE)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Y2whJsh77Tje"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look at an entry in validation dataset."
      ]
    },
    {
      "metadata": {
        "id": "c4XLMq4g7Tje"
      },
      "cell_type": "code",
      "source": [
        "SOURCE_EXAMPLE_DS = SOURCE_VALIDATION_DS.take(1).batch(1, drop_remainder=True)\n",
        "SOURCE_EXAMPLE = SOURCE_EXAMPLE_DS.as_numpy_iterator().next()\n",
        "for key, val in SOURCE_EXAMPLE.items():\n",
        "  decoded_val = val[0].decode('utf-8')\n",
        "  print(f'{key}:\\n\"{decoded_val}\"\\n')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "VRANVuuL7lnZ"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-process the data to the expected format\n",
        "\n",
        "Gemma3 expects the training examples in the format: `{\"prompts\": list[str], \"responses\": list[str]}`, where `prompts[i]` and `response[i]` are the i-th prompt and its expected response.\n",
        "\n",
        "We construct the prompt for each example in the following way:\n",
        "```\n",
        "Summarize the following dialogue:\n",
        "{dialogue}\n",
        "Summary:\n",
        "```\n",
        "\n",
        "\n",
        "We prepend a prefix and a suffix to the dialogue to make the prompt more self-explanatory.\n",
        "\n",
        "The response is added without any prefixes and suffixes, just the summary without any additional text.\n",
        "\n",
        "Such a format of input to Gemma3 model means that we will train the model in the following way:\n",
        "\n",
        "given a prompt in the format above it has to generate the following text:\n",
        "```\n",
        "Summarize the following dialogue:\n",
        "{dialogue}\n",
        "Summary:\n",
        "{summary}\n",
        "```\n",
        "\n",
        "I.e. expressing it in Python it has to generate `prompt + summary`."
      ]
    },
    {
      "metadata": {
        "id": "y7WYwDQe7Tje"
      },
      "cell_type": "code",
      "source": [
        "def source_to_gemma3_format(dialogue_dict):\n",
        "  return {\n",
        "      \"prompts\": tf.strings.join([\n",
        "          \"Summarize the following dialogue:\\n\",\n",
        "          dialogue_dict[\"dialogue\"],\n",
        "          \"\\nSummary:\\n\",\n",
        "      ]),\n",
        "      \"responses\": dialogue_dict[\"summary\"],\n",
        "  }"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "0Ll8Of6I7Tje"
      },
      "cell_type": "code",
      "source": [
        "TRAIN_DS = SOURCE_TRAIN_DS.map(source_to_gemma3_format)\n",
        "VALIDATION_DS = SOURCE_VALIDATION_DS.map(source_to_gemma3_format)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "4TkTKziI7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the input to our model."
      ]
    },
    {
      "metadata": {
        "id": "Y66nZVr97Tje"
      },
      "cell_type": "code",
      "source": [
        "EXAMPLE_DS = VALIDATION_DS.take(1).batch(1, drop_remainder=True)\n",
        "EXAMPLE = EXAMPLE_DS.as_numpy_iterator().next()\n",
        "for key, val in EXAMPLE.items():\n",
        "  decoded_val = val[0].decode('utf-8')\n",
        "  print(f'{key}:\\n\"{decoded_val}\"\\n')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "DtzAq-vi7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Determine training size\n",
        "\n",
        "We need to determine the training set size because it directly impacts the total number of optimization steps. The number of optimization steps is precisely determined by the interplay of the training set size, the number of epochs, the batch size, and the gradient accumulation steps.\n",
        "\n",
        "Knowing the exact number of optimization steps beforehand is essential for calibrated noise generation in DP-SGD. During each optimization step, noise is added to ensure a specific protection level (defined by epsilon and delta). To accurately calibrate this noise for the desired privacy guarantees, we must know precisely how many times the noise will be generated throughout the training process."
      ]
    },
    {
      "metadata": {
        "id": "_1yhT03u7Tje"
      },
      "cell_type": "code",
      "source": [
        "# Train size is important for DP-SGD.\n",
        "TRAIN_SIZE = int(TRAIN_DS.cardinality().numpy())\n",
        "print(f'Train size: {TRAIN_SIZE}')\n",
        "VALIDATION_SIZE = int(VALIDATION_DS.cardinality().numpy())\n",
        "print(f'Validation size: {VALIDATION_SIZE}')\n",
        "\n",
        "TRAIN_DS = TRAIN_DS.shuffle(buffer_size=2048).batch(\n",
        "    BATCH_SIZE, drop_remainder=True\n",
        ")\n",
        "VALIDATION_DS = VALIDATION_DS.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "WmmS8JKS7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup data parallelism\n",
        "\n",
        "Parallelize training on all available devices, by splitting data by batch dimension."
      ]
    },
    {
      "metadata": {
        "id": "7DvtI4VZ7Tje"
      },
      "cell_type": "code",
      "source": [
        "DATA_PARALLEL = keras.distribution.DataParallel()\n",
        "# You can see over how many GPUs the data will be distributed.\n",
        "print(DATA_PARALLEL)\n",
        "keras.distribution.set_distribution(DATA_PARALLEL)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "AbDWkmgi7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "## Gemma3 model setup"
      ]
    },
    {
      "metadata": {
        "id": "GANRRbbY7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the model\n",
        "\n",
        "If `USE_MIXED_PRECISION` is true then model will be loaded with 16-bit weights.\n",
        "\n",
        "It is important that Gemma3 preprocessor is `Gemma3CausalLMPreprocessor` because it does masking and padding properly (assuming the input is in the `{prompts, responses}` format)."
      ]
    },
    {
      "metadata": {
        "id": "F6N32c5G7Tje"
      },
      "cell_type": "code",
      "source": [
        "MODEL_WEIGHTS_DTYPE = None  # use default dtype\n",
        "if USE_MIXED_PRECISION:\n",
        "  print(\"Using mixed precision\")\n",
        "  keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
        "  MODEL_WEIGHTS_DTYPE = \"bfloat16\"\n",
        "\n",
        "gemma_lm = keras_hub.models.Gemma3CausalLM.from_preset(\n",
        "    GEMMA3_MODEL_TYPE, dtype=MODEL_WEIGHTS_DTYPE\n",
        ")\n",
        "\n",
        "assert isinstance(\n",
        "    gemma_lm.preprocessor, keras_hub.models.Gemma3CausalLMPreprocessor\n",
        ")\n",
        "gemma_lm.preprocessor.sequence_length = SEQUENCE_LENGTH\n",
        "gemma_lm.summary()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "AfDluR387Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Validation example inference before fine-tuning\n",
        "\n",
        "Let's see what the model outputs before we fine-tune it."
      ]
    },
    {
      "metadata": {
        "id": "esJ92gB_7Tje"
      },
      "cell_type": "code",
      "source": [
        "def make_validation_example_inference():\n",
        "  return gemma_lm.generate(EXAMPLE[\"prompts\"])[0]\n",
        "\n",
        "\n",
        "def show_validation_example_inference():\n",
        "  print(make_validation_example_inference())\n",
        "  print(f\"\\nCorrect summary:\\n{EXAMPLE['responses'][0].decode('utf-8')}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "pQ0cEYLT7Tje"
      },
      "cell_type": "code",
      "source": [
        "show_validation_example_inference()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3c0T1xEo7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "Not too bad but we can see the artifacts which fine-tuning can help us get rid of: for example the model always outputs `<end_of_turn>`."
      ]
    },
    {
      "metadata": {
        "id": "AB8PKUoC7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Enable LoRA\n",
        "\n",
        "Notice how the number of trainable params significantly decreased. It happens so because the LoRA rank defines the size of trainable matrices and our rank is quite small. This is how LoRA makes fine-tuning of LLMs a reality! You can learn more about LoRA [here](https://ai.google.dev/gemma/docs/core/lora_tuning#configure_lora_tuning)."
      ]
    },
    {
      "metadata": {
        "id": "UEiwDv5A7Tje"
      },
      "cell_type": "code",
      "source": [
        "gemma_lm.backbone.enable_lora(rank=LORA_RANK)\n",
        "gemma_lm.summary()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "E_pLUyx_7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Enable DP\n",
        "\n",
        "The most interesting part of this notebook!\n",
        "\n",
        "If you want to do DP-SGD training you have to create `DPKerasConfig` providing to JAX Privacy essential parameters for DP training. There is nothing surprising in the config values, we've already defined everything in the preceeding cells. You can notice, for example, how we use `TRAIN_SIZE` we determined earlier to calculate the number of train steps. Also note that `gradient_accumulation_steps` is provided as a separate parameter. JAX Privacy takes it into account to calculate the real number of optimization steps during the training.\n",
        "\n",
        "JAX Privacy will throw an exception if we exceed the specified number of train steps. If we do fewer train step then we won't consume all the (eps, delta)-DP budget and add more noise than necessary. Therefore always make these params tight and rerun training if you realize that you don't need that many training steps.\n",
        "\n",
        "Once params config is created, we have to call `make_private` providing the model and params. It will return an updated model whose further training for the pre-defined number of training steps will be differentially-private, you don't have to do anything more.\n",
        "\n",
        "During these calls `noise multiplier` will be calculated: this is standard deviation of the Gaussian noise that will be added to the accumulated gradient at each optimization step. The value of it will be printed to STDOUT."
      ]
    },
    {
      "metadata": {
        "id": "GE3iiTkh7Tje"
      },
      "cell_type": "code",
      "source": [
        "if USE_DP:\n",
        "  params = keras_api.DPKerasConfig(\n",
        "      epsilon=EPSILON,\n",
        "      delta=DELTA,\n",
        "      clipping_norm=CLIPPING_NORM,\n",
        "      batch_size=BATCH_SIZE,\n",
        "      train_steps=EPOCHS * (TRAIN_SIZE // BATCH_SIZE),\n",
        "      train_size=TRAIN_SIZE,\n",
        "      gradient_accumulation_steps=GRADIANT_ACCUMULATION_STEPS,\n",
        "      seed=SEED,\n",
        "  )\n",
        "  gemma_lm = keras_api.make_private(gemma_lm, params)\n",
        "  print(f\"DP training: {CLIPPING_NORM=} {EPOCHS=} {BATCH_SIZE=}\")\n",
        "else:\n",
        "  print(\"Non-DP training\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "_H-_gEsm7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare the model for training\n",
        "\n",
        "Create optimizer, providing learning rate and accumulation steps. Then compile the model for training."
      ]
    },
    {
      "metadata": {
        "id": "MwVuFmlp7Tje"
      },
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    gradient_accumulation_steps=GRADIANT_ACCUMULATION_STEPS,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SOPdaX-u7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "## Let's DP fine-tune Gemma3!"
      ]
    },
    {
      "metadata": {
        "id": "pEVdoWcO7Tje"
      },
      "cell_type": "code",
      "source": [
        "gemma_lm.fit(x=TRAIN_DS, epochs=EPOCHS, validation_data=VALIDATION_DS)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "siGKXRha7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**: You can't call `fit` anymore because you've already performed the maximum allowed number of training steps. If you try to do more model optimization (training) steps, an exception will be thrown because otherwise you will exceed your declared (eps, delta)-DP budget."
      ]
    },
    {
      "metadata": {
        "id": "sk5wZ7YY7Tje"
      },
      "cell_type": "markdown",
      "source": [
        "### Validation example inference after fine-tuning\n",
        "\n",
        "Let's see what the model outputs now, after we fine-tuned it."
      ]
    },
    {
      "metadata": {
        "id": "jPFcFyKv7Tje"
      },
      "cell_type": "code",
      "source": [
        "show_validation_example_inference()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-D_fyyAh7Tjf"
      },
      "cell_type": "markdown",
      "source": [
        "Now it performs better. For example, there is no `<end_of_turn>` in the output."
      ]
    },
    {
      "metadata": {
        "id": "E-15P--X7Tjf"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate performance metrics on the test dataset\n",
        "\n",
        "Let's calculate F1 score of ROUGE metrics on the test dataset that our model hasn't seen yet. We will use these metrics to compare different setups between each other.\n",
        "\n",
        "It is interesting to see performance of:\n",
        "\n",
        "* the non-fine-tuned model (baseline)\n",
        "* non-DP fine-tuned model\n",
        "* DP fine-tuned model"
      ]
    },
    {
      "metadata": {
        "id": "ZdaSfiUV7Tjf"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare the model for testing\n",
        "\n",
        "You might want to increase the maximum sequence length for testing to make all test examples fit this length (2048 is more than enough). We use 512, not all examples fit into this length but majority (~95%) of them do, so it is fine.\n",
        "\n",
        "Note that the length is in Gemma3 tokens and not in number of English words. So, to evaluate how many of examples will be truncated you need to use `gemma_lm.preprocessor.tokenier.tokenize(str)`."
      ]
    },
    {
      "metadata": {
        "id": "1H5FVFD27Tjf"
      },
      "cell_type": "code",
      "source": [
        "gemma_lm.preprocessor.sequence_length = TEST_DS_SEQUENCE_LENGTH"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "uyNFwQ1e7Tjf"
      },
      "cell_type": "markdown",
      "source": [
        "### Load and pre-process the test dataset\n",
        "\n",
        "Following the same process we did it for the train and validation datasets."
      ]
    },
    {
      "metadata": {
        "id": "gaM8Ukvw7Tjf"
      },
      "cell_type": "code",
      "source": [
        "SOURCE_TEST_DS = tfds.load(\"samsum\", split=\"test\")\n",
        "TEST_DS = SOURCE_TEST_DS.map(source_to_gemma3_format)\n",
        "TEST_SIZE = int(TEST_DS.cardinality().numpy())\n",
        "print(f\"Test size: {TEST_SIZE}\")\n",
        "TEST_DS = TEST_DS.batch(TEST_DS_BATCH_SIZE)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "tjYpZrXu7Tjf"
      },
      "cell_type": "markdown",
      "source": [
        "### List the evaluation metrics\n",
        "\n",
        "We will calculate ROUGE_1, ROUGE_2 and ROUGE_L metrics. See [this guide](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499) if you don't know what these metrics mean."
      ]
    },
    {
      "metadata": {
        "id": "iahvBOrN7Tjf"
      },
      "cell_type": "code",
      "source": [
        "METRIC_FNS = {\n",
        "    'rouge_1': keras_hub.metrics.RougeN(order=1),\n",
        "    'rouge_2': keras_hub.metrics.RougeN(order=2),\n",
        "    'rouge_l': keras_hub.metrics.RougeL(),\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "J7RvxOJ67Tjf"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluation code\n",
        "\n",
        "For each test example, we should feed only the prompt without the reponse to the model. The model will output the text that will contain the prompt and the generated summary, therefore we have to remove the prompt from the text. After that we have to take the expected response (summary) and supply both the generated summary and the expected summary to the ROUGE metrics library that will compare two strings and will calculate the ROUGE metrics for us."
      ]
    },
    {
      "metadata": {
        "id": "FbelO_KR7Tjf"
      },
      "cell_type": "code",
      "source": [
        "def calculate_common_prefix(str1, str2):\n",
        "  i = 0\n",
        "  while i < len(str1) and i < len(str2) and str1[i] == str2[i]:\n",
        "    i += 1\n",
        "  return i\n",
        "\n",
        "\n",
        "def strip_prompts_from_outputs(\n",
        "    prompts: list[str], generated_outputs: list[str]\n",
        ") -> list[str]:\n",
        "  stripped_outputs = []\n",
        "\n",
        "  for prompt, full_output in zip(prompts, generated_outputs):\n",
        "    # Find the first position of the prompt in the output and strip it.\n",
        "    common_prefix = calculate_common_prefix(prompt, full_output)\n",
        "    stripped_outputs.append(full_output[common_prefix:])\n",
        "\n",
        "  return stripped_outputs\n",
        "\n",
        "\n",
        "def eval_batch(batch):\n",
        "  prompts = [p.decode(\"utf-8\") for p in batch[\"prompts\"].numpy()]\n",
        "  # Important: do not feed responses to the model, supply only prompts.\n",
        "  output_batch = gemma_lm.generate(prompts)\n",
        "  output_text = strip_prompts_from_outputs(prompts, output_batch)\n",
        "  target_text = [s.decode(\"utf-8\") for s in batch[\"responses\"].numpy()]\n",
        "\n",
        "  for _, metric_fn in METRIC_FNS.items():\n",
        "    metric_fn.update_state(target_text, output_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "RS6j-pYDz3UW"
      },
      "cell_type": "markdown",
      "source": [
        "### Let's calculate the metrics!\n",
        "\n",
        "For each ROUGE metric we can calculate precision and recall. To evaluate both of them in one number we take F1 score."
      ]
    },
    {
      "metadata": {
        "id": "n8A6NTU-z3UW"
      },
      "cell_type": "code",
      "source": [
        "for batch in tqdm.tqdm(TEST_DS):\n",
        "  eval_batch(batch)\n",
        "\n",
        "RESULT = {f'{k}': m.result()['f1_score'] for k, m in METRIC_FNS.items()}\n",
        "print(RESULT)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "EArMmpREt0pC"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "For Gemma3 4b tuned for instructions (`gemma3_instruct_4b_text`) you can expect the following F1 scores on test dataset:\n",
        "\n",
        "| Experiment | ROUGE_1 | ROUGE_2 | ROUGE_L |\n",
        "|---|---|---|---|\n",
        "| Baseline (no fine-tuning) | 0.341 | 0.127 | 0.263 |\n",
        "| Non-DP fine-tuning (i.e. `USE_DP=False`) | 0.512 | 0.273 | 0.433 |\n",
        "| DP fine-tuning (i.e. `USE_DP=True`) | 0.487 | 0.251 | 0.412 |\n",
        "\n",
        "The rest of the hyper-parameters were the same as in the setup cell in the beggining of the notebook, i.e.:\n",
        "\n",
        "```\n",
        "Variable                      Type        Data/Info\n",
        "---------------------------------------------------\n",
        "BATCH_SIZE                    int         16\n",
        "CLIPPING_NORM                 float       0.001\n",
        "DELTA                         float       2e-05\n",
        "EPOCHS                        int         3\n",
        "EPSILON                       float       4.0\n",
        "GEMMA3_MODEL_TYPE             str         gemma3_instruct_4b_text\n",
        "GRADIANT_ACCUMULATION_STEPS   int         64\n",
        "LEARNING_RATE                 float       0.003\n",
        "LORA_RANK                     int         32\n",
        "SEED                          int         0\n",
        "SEQUENCE_LENGTH               int         512\n",
        "TEST_DS_BATCH_SIZE            int         16\n",
        "TEST_DS_SEQUENCE_LENGTH       int         512\n",
        "TEST_RUN                      bool        False\n",
        "USE_MIXED_PRECISION           bool        False\n",
        "```\n",
        "\n",
        "These results were obtained on 16 A100 40GB GPUs."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "last_runtime": {
        "build_target": "//research/colab/notebook:notebook_backend_ephemeral",
        "kind": "private"
      },
      "name": "dp_sgd_keras_gemma3_lora_finetuning_samsum.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
